<div>
<div align='center'>
<h2 align="center"> ViPE: Visualise Pretty-much Everything </h2>
<h3 align="center"></h3>
</div>
<div>
<div align="center">
    <a href='https://fittar.me/' target='_blank'>Hassan Shahmohammadi<sup>&#x2709</sup></a>&emsp;
    <a href='https://adhirajghosh.github.io/' target='_blank'>Adhiraj Ghosh</a>&emsp;
    <a href='https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/mitarbeiter/prof-dr-ing-hendrik-lensch/' target='_blank'>Hendrik PA Lensch</a>&emsp;
    </br>
</div>
<div align="center">
    <a href='https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/computergrafik/lehrstuhl/'> Computer Graphics Group, University of TÃ¼bingen </a>&emsp;
    </br>
    <sup>&#x2709</sup> Corresponding Author


<div style="text-align:center">
<img src="./media/teaser.png"  width="50%" height="50%">
</div>

### [Project Page](https://adhirajghosh.github.io/) | [ViPE Paper](https://arxiv.org/abs/2305.03726)  | [LyricCanvas Dataset](https://adhirajghosh.github.io/)

<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>

</div>

TODO: 
1. Correct all the links. 
2. Add Video generation website 
3. Add HuggingFace page if available
4. Refactor chatgpt, chatgpt_run and genius to one folder called lyric_canvas?
5. Remove .sh files
6. When we deploy, we should change to Hazel1994/vipe
 -----------------
## ğŸ¦¾ Updates
## ğŸ—„ Code Structure

```
â”œâ”€â”€ vipe
â”‚   â”œâ”€â”€ chatgpt-run                   <- build your own LLM-powered dataset
â”‚   â”œâ”€â”€ datasets                      <- path to all relevant datasets to reproduce ViPE results
â”‚   â”œâ”€â”€ genius                        <- implement the genius API
â”‚   â”‚â”€â”€ README.md                    
â”‚   â””â”€â”€ output                        <- folder that stores models and logs
â”‚
```

## ğŸ’¾ Downloads
TODO:
1. Path to the retrieval files. All 4 pickle files and the images for train and eval. Upload to cloud.

### HAIVMet
We stack ViPE against human annotators in understanding and visualising figurative speech. To that end, we refer to [VisualMetaphors](https://github.com/tuhinjubcse/VisualMetaphors). To download the dataset, please follow their instructions.    

The ```datasets``` folder should have the following structure
```
â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ HAIVMet
â”‚   â”‚   â”‚â”€â”€ ad_slogans.zip   
â”‚   â”‚   â”‚â”€â”€ bizzoni.zip
â”‚   â”‚   â”‚â”€â”€ copoet.zip
â”‚   â”‚   â”‚â”€â”€ figqa.zip
â”‚   â”‚   â”‚â”€â”€ flute.zip
â”‚   â”‚   â”‚â”€â”€ tsevtkov.zip
â”‚   â”œâ”€â”€ retrieval
â”‚   â”‚   â”‚â”€â”€ chatgpt  
â”‚   â”‚   â”‚â”€â”€ haivmet   
â”‚   â”‚   â”‚â”€â”€ vipe   
â”‚   â”‚   â”‚â”€â”€ metaphor_id.pickle   
â”‚   â”‚   â”‚â”€â”€ prompt_dict_chatgpt.pickle  
â”‚   â”‚   â”‚â”€â”€ prompt_dict_haivmet.pickle  
â”‚   â”‚   â”‚â”€â”€ prompt_dict_vipe.pickle  
```

## Evaluation
### Image-Text Retrieval
To generate datasets for the respective models, run the following:
```bash
python3 evaluation/retrieval/create_dataset.py --model <haivmet/vipe/chatgpt> \
--dataset <ad_slogans/bizzoni/copoet/figqa/flute/tsvetkov>\
--savedir <path/to/store/datasets/>\
--img_size <image resolution> --num_images <number of images per prompt>\
--checkpoint <path/to/vipe/checkpoint/if/using/vipe>
```
We conduct vigorous image-text retrieval using the [BLIP model](https://github.com/salesforce/BLIP) as the benchmark model. 
```bash
python3 evaluation/retrieval/evaluation.py --dataset <haivmet/vipe/chatgpt> --output_dir <path/to/store/checkpoints> --id_type <metaphor/prompt>
```
## ğŸ“¹ Music Video Generation
Music Video Generation strategy as used in our paper. For an updated version, please refer to [ViPE-Videos](https://github.com/Hazel1994/ViPE-Videos).
```bash
python3 ./t2v/create_video.py --img_size 100 --outdir ./results/vids/finalise/ --fps 2
```

## ğŸ“‘ Citation

If you found this repository useful, please consider citing:

## ğŸ‘¨â€ğŸ« Acknowledgements
We refer to portions of the implementations of the following for parts of our research: 

- [Text2Video-Zero](https://github.com/Picsart-AI-Research/Text2Video-Zero)
